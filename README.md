# LLM
Large Language Models coupled with multiple AI capabilities to generate images and text as well as achieving human level performance on a number of tasks, the world is going through revolution in art (DALL-E, MidJourney, Imagine, etc.), science (AlphaFold), medicine, and other key areas.  In-context learning, popularized by the team behind the GPT-3 LLM, brought a new revolution for using LLMs in so many tasks that the LLM is not trained for. This is in contrast to the usual fine-tuning that was required to equip AI models to perform better in tasks they are not trained for.  With In-context learning, LLMs are able to readjust their performance on a task depending on the prompt - from structured input that can be considered partly a few-shot training and partly a test input. This has opened up many applications. This weekâ€™s challenge is to systematically explore strategies that help generate prompts for LLMs to do classification of web pages according to a few examples of human scores. You will be also required to compare responses and accuracies of multiple LLM models for a given promp
